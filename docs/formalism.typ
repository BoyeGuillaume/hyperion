#import "@preview/diatypst:0.8.0": *
#import "@preview/thmbox:0.3.0": *
#import "@preview/rustycure:0.2.0": qr-code
#import "@preview/diagraph:0.3.6": raw-render
#import "@preview/fletcher:0.5.8" as fletcher: diagram, edge, node

#let blob(pos, label, tint: white, ..args) = node(
  pos,
  align(center, label),
  width: 28mm,
  fill: tint.lighten(60%),
  stroke: 1pt + tint.darken(20%),
  corner-radius: 5pt,
  ..args,
)
#let with_alpha(c, a) = rgb(..c.components().slice(0, 3), a)
#show: thmbox-init()

// #let diapositive-break() = {}
#let diapositive-break() = pagebreak()
#show: slides.with(
  title: "Hyperion: Technical Overview",
  subtitle: "Presentation of hyperion core",
  date: {
    datetime.today().display("[day] [month repr:long] [year repr:full]")
  },
  authors: "Guillaume BoyÃ©",

  // Optional styling
  ratio: 16 / 9,
  layout: "medium",
  title-color: blue.darken(60%),
  theme: "full",
  toc: true,
  count: "number",
)

#let todo = thmbox.with(
  color: colors.dark-red,
  variant: translations.variant("Todo"),
  numbering: none,
  sans: false,
  fill: colors.dark-red.lighten(90%),
)

= Formalism

== Instruction set

#v(5mm)

Much like compilers, hyperion at its core relies on an *intermediate representation* of programs, called the hyperion IR. Taking inspiration from _LLVM IR_#footnote[https://llvm.org/docs/LangRef.html], hyperion IR is a low-level typed assembly-like language that is platform-agnostic.

#v(5mm)

It's key features are:
- *SSA* (Static Single Assignment) form, meaning each variable is assigned exactly once.
- *Typed* instructions, meaning each instruction has a well-defined type.
- Seamless support for *SIMD* operations, allowing vectorized computations.
- Phi-nodes for control flow merging.

As mention, it is design for *intercompatibility* with _LLVM_ to ease compilation to native code.

== Instruction set -- Example

Here is a simple example of a hyperion IR function that returns the integer `1`:

```rust
let a_block = BasicBlock {
  label: Label::NIL, // entry block
  instructions: vec![todo!()],
  terminator: Terminator::Return {
    value: Some(Operand::Imm(1u32.into())),
  }.into(),
}

let function = Function {
  parameters: vec![],
  return_type: Type::U32,
  basic_blocks: vec![a_block],
  /// Other fields...
};
```

== Instruction set -- Formalism

We want to define a formal semantics of functions mathematically.

#definition(title: "Proposal 1")[
  A function $f in bb(F)$ is a mapping from arguments $A_1,..., A_n$ to $O$.

  Where:
  - $A_i$ are the argument types.
  - $O$ is the output type.
]

*Limitation*:
- *Memory* is limited requiring our model to account for memory state. This model cannot account for side-effects.

#diapositive-break()
Let us then add output memory state to the definition.

#definition(title: "Proposal 2")[
  A function $f in bb(F)$ is defined as a mapping $f in (A_1, ..., A_n, Gamma) -> O times Gamma$

  Where:
  - $A_i$ are the argument types.
  - $O$ is the output type.
  - $Gamma$ represents the memory state.
]

*Limitation*:
- Does not account for non-terminating/crashing functions.
- Does not account for concurrency.

#diapositive-break()
We therefore attempt another definition that account for non-termination and crashes.

#definition(title: "Proposal 3")[
  A function $f in bb(F)$ is defined as a mapping $f in (A_1, ..., A_n, Gamma) -> (O union {lozenge.filled, lozenge}) times Gamma$

  Where:
  - $A_i$ are the argument types.
  - $O$ is the output type.
  - $Gamma$ represents the memory state.
  - $lozenge.filled$ represents non-termination.
  - $lozenge$ represents a crash.
]

*Limitation*:
- Does not account for *concurrency*.
- Lacks finer modeling of *memory interaction* (notably *concurrency* and *independence*)

#diapositive-break()
Let us define a new notion called temporally-dependent function

#definition(title: "Proposal 4")[
  A function $f in FF$ defined as an arbitrary object. Let us define the transition function $tau_f(a_1, ..., a_n) : Gamma times Theta -> Gamma times (Theta union \{ lozenge.filled, lozenge \})$. We can then define the
  following series~:
  - $(theta_i, Gamma^f_0) = ({a_1, ..., a_n}, gamma)$
  - $(theta_(i+1), Gamma^f_(i+1)) = tau_f(a_1, ..., a_n)(theta_i, Gamma^f_i)$
  We call $Gamma$ the *memory trace* of $f$ on inputs $a_1, ..., a_n$ and initial memory state $gamma$. Concurrency can be introduce in which case the value of $Gamma^f_i$ depend on the interleaving of memory operations between multiple functions.
]

#definition(title: "Runtime")[
  We define the *runtime of a trace* generated by $f$ on arguments $a_1...a_n$ as $epsilon{f}(a_1, ..., a_n)$
  $
    epsilon{f}(a_1, ..., a_n) ="min" {i in NN | Gamma^f_i in {lozenge.filled, lozenge}}
  $
  Notice that this definition only allow runtime of *non-concurrent* functions. That is to say functions or sets of functions that are executed in isolation. We generalize this *runtime* to a set of concurrent function $F = {f_1, ..., f_m}$ as~:
  $
    epsilon{F}(a_(1,1), ..., a_(n,m)) = max_j min_i {i in NN | Gamma^(f_j)_i in {lozenge.filled, lozenge}}
  $
  If the maximum/minimum does not exists, then by convention we set $epsilon{f}(a_1, ..., a_n) = lozenge.filled$. Because $epsilon$ is
  undecidable in the general case, we also allow $epsilon{f}(a_1, ..., a_n) = top$ to represent an *unknown* runtime.
]

#diapositive-break()
We choose *Proposal 3* as our formal model for hyperion functions. Additionally, for any function $f$ we define the following~:

#definition(title: "Memory/function interaction")[
  - The *observed memory* of $f$ is defined as the set of memory locations read/written by $f$, during it's execution trace noted $delta \{f\}(a_1, ..., a_n, gamma)$.
  - The *modified memory* of $f$ is defined as the set of memory locations written by $f$, during it's execution trace noted $mu \{f\}(a_1, ..., a_n, gamma)$.
]

#note[
  We have that for any function $f$ and any arguments $a_1, ..., a_n, gamma$~:
  $
    delta{f}(a_1, ..., a_n, gamma) supset mu{f}(a_1, ..., a_n, gamma)
  $
]

== Instruction set -- A note on concurrency
Concurrency is crutial to model due to the inherent limitation of _single-threaded models_. However in concurrency, state of memory is shared between potentially multiple functions $f$ and we need to consider the *resulting operations*.

#definition(title: "Non interfering")[
  We say that two functions $f$ and $g$ are *non-interfering* if for any inputs, the *observed memory* of $f$ does not intersect with the *modified memory* of $g$ and vice-versa~:
  $
    forall a_1, ..., a_n in A_1 times ... times A_n, b_1, ..., b_m in B_1 times ... times B_m forall gamma in Gamma. \
    (delta \{f\}(a_1, ..., a_n, gamma) inter mu \{g\}(b_1, ..., b_m, gamma) union \
    (delta \{g\}(b_1, ..., b_m, gamma) inter mu \{f\}(a_1, ..., a_n, gamma)) = emptyset
  $
]
#text(size: 0.8em)[
  #note[
    Actual definition provides stronger guarantees notably on all concurrent of executions of $f$ and $g$ satisfy this property.
  ]
]

#diapositive-break()

#v(5mm)

In hyperion, we have that if two functions $f$ and $g$ are executed concurrently then one of the following holds~:
- $f$ and $g$ are non-interfering,
- We can define a *synchronization mechanism* to ensure that we can split $f$ and $g$ into a series of non-interfering or non-concurrent sub-functions.

  In this case, we need to optimize the entire program/system to ensure that the synchronization mechanism is respected.

== Analysis of a function

Given two functions $f, g in bb(F)$ we say that $f$ and $g$ are *equivalent* if for any inputs~:
$
  forall a_1, ..., a_n in A_1 times ... times A_n, gamma in Gamma. \
  f(a_1, ..., a_n, gamma) = g(a_1, ..., a_n, gamma) and \
  delta \{f\}(a_1, ..., a_n, gamma) = delta \{g\}(a_1, ..., a_n, gamma)
$

Furthermore, we say that $f$ is *strongly equivalent* to $g$ (noted $f arrow.l.r.wave g$) if for any program using $f$, replacing $f$ with $g$ results in an observable-equivalent program.

_Note_: Strong equivalence implies equivalence, but the converse is not necessarily true. *Strongly equivalent* provide guarantees about the order of memory operations remain identic between $f$ and $g$.

We say that $f$ and $g$ are *(strongly) equivalent under $C(a_1, ..., a_n, Gamma)$* (noted $f arrow.l.r.wave_C g$) iif $f$ and $f'$ are (strongly) equivalent where:
$
  f'(a_1, ..., a_n, gamma) := cases(
    g(a_1, ..., a_n, gamma) "if" C(a_1, ..., a_n, gamma),
    f(a_1, ..., a_n, gamma) "otherwise"
  )
$

#diapositive-break()
We define the *optimization* problem as follow~:

Given a program $P$ using a set of functions $F = {f_1, ..., f_n}$, and
and oracle $cal(O)$ mapping $cal(O) : cal(P) times cal(A) times cal(Gamma) -> bb(R)$. Find a
set of functions $F' = {f'_1, ..., f'_n}$ such that~:
- $forall 1 <= i <= n.space f_i arrow.l.r.wave f'_i$
- $F' = "argmin"_F space EE_(a, gamma ~ cal(A) times Gamma) (cal(O)(P_F', a, gamma)$)

As such the *goal* of this framework is to explore the set of possible functions $F'$ that are (strongly) equivalent to $F$ and find the set that optimizes the oracle $cal(O)$.

= Optimization
== State of optimization -- Definitions
We define the *state of optimization* as follow~:
#definition(title: "CFG")[
  Given a function $f in bb(F)$, made of basic blocks $B = {b_1, ..., b_m}$, we define the *control flow graph* of $f$ as a directed graph $"CFG"(f) = (V, E)$ where~:
  - $V = b_i$
  - $E subset V times V$ where $(b_i, b_j) in E$ if there exists a possible execution path from $b_i$ to $b_j$.
]

#definition(title: "Reachability")[
  Given a function $f in bb(F)$, made of basic blocks $B = {b_1, ..., b_m}$, we say that a basic block $b_j$ is *reachable* from a basic block $b_i$ if there exists a path from $b_i$ to $b_j$ in the control flow graph $"CFG"(f)$.
]

== State of optimization -- A Simple Example
Suppose we have the following function

#text(size: 0.8em)[
  ```ll
  define i32 @pow(i32 %base, i32 %exp) {
  entry:
    %is_zero = icmp eq i32 %exp, 0 ; Check if exponent is zero
    br i1 %is_zero, label %output, label %loop ; Branch based on exponent

  loop: ; Main loop for exponentiation
    %current = phi i32 [1, %entry], [%next, %loop]
    %current_exp = phi i32 [%exp, %entry], [%next_exp, %loop]
    %next_exp = sub i32 %current_exp, 1 ; Decrement exponent
    %next = mul i32 %current, %base ; Multiply current result by base
    %is_done = icmp eq i32 %next_exp, 0 ; Check if exponentiation is done
    br i1 %is_done, label %output, label %loop ; Branch based on completion

  output:
    %result = phi i32 [1, %entry], [%next, %loop] ; Final result
    ret i32 %result ; Return the result
  }
  ```
]

#diapositive-break()
We perform the following analysis:

We perform a *simple-disjunction* to determine based on the follow CFG~:
#align(center, {
  diagram(
    cell-size: (50mm, 9mm),
    spacing: (24pt, 5pt),
    // spacing: (8pt, 1pt),
    mark-scale: 70%,

    blob((0, 0), [Entry], tint: yellow, shape: fletcher.shapes.hexagon),
    blob((1, 0), [Loop], tint: orange),
    blob((2, 0), [Output], tint: green, shape: fletcher.shapes.hexagon),

    edge((0, 0), (1, 0), text($"%exp" != 0$), "-|>"),
    edge((1, 0), (2, 0), text($"%next_exp" = 0$), "-|>"),
    edge((1, 0), (1, 0), text($"%next_exp" != 0$), "-|>", bend: 135deg),
    edge((0, 0), (2, 0), text($"%exp" = 0$), "-|>", bend: -13deg),
  )
  // raw-render(
  //   ```dot
  //   digraph {
  //     node [shape=circle, style=filled, fillcolor=lightgrey];
  //     rankdir="LR";

  //     entry -> loop;
  //     loop -> loop;
  //     loop -> output;
  //     entry -> output;
  //   }
  //   ```,
  // )
})

We split the function into two possible paths~:
- Path 1: `entry -> output` when `exp == 0`
- Path 2: `entry -> loop* -> output` when `exp != 0`
This mean we add two entries with two sets of precondition. *We merge them later*.

#diapositive-break()
Path 1 is trivial, as such we will only analyse path 2.

*Step 1*: Determine reachability of condition `is_done`:
- Reachability same as `%next_expr == 0`
- Relise that `%next_expr` is a `loop covariant` and depend on `%exp`
- Simplify it (only consider element directly dependent and relize that `%next_expr` is decremented by `1` each loop)
- Arithmetic progression, we relize that `is_done` is reachable when `%exp >= 1`
Conclusion of *step 1*:
1. $1 <= %"current_exp" <= %"exp"$
2. `is_done` is *always* reachable when $%"exp" >= 1$
3. Number of loop iteration is exactly $%"exp"$
4. At loop exit, we have that $%"next_exp" == 0$

#diapositive-break()
*Step 2*: Determine value of `%current` at loop exit:
- We relize that `%current` is multiplied by `%base` each loop iteration
- We have that `%current` starts at `1` and is multiplied by `%base` exactly `%exp` times
Conclusion of *step 2*:
1. At loop exit, we have that `%current == %base * %base * ... * %base` (`%exp` times)

*Step 3*: Additional properties derivation (*WIP*):
- We can derive that `%pow(%base, %a + %b) == %pow(%base, %a) * %pow(%base, %b)`
- We can derive that `%pow(%a * %b, %exp) == %pow(%a, %exp) * %pow(%b, %exp)`
- We can derive that `%pow(%base, %0) == 1`

#todo[
  How can we automate the derivation of such properties ?
  - Invariant and properties also need to track timing (for instance for container get/set properties)
]

== State of optimization -- Determinism

What could be the post-condition for non-deterministic functions ?
```ll
declare i32 @nondet random();
```

Many algorithm relies on non-determinism to provide better average-case performance. This underline the
need to model non-determinism in our framework. To do so we have a simple approach
- Define the *probability* function $P : cal(B) -> RR_[0,1]$ where $cal(B)$ is the set of booleans.
- Define the *expected value* function $EE : O -> RR$ where $O$ is the set of integers/reals.
- Define the *variance* function $"Var" : O -> RR_+$ where $O$ is the set of integers/reals.

Introduce a special label `#L` that represent number of times a label is reached during execution. For instance~:
$
  P("reachable"(L)) = P("#"L >= 1) \
  epsilon(f) = sum_i EE("#" B_i) dot "size"(B_i) " for deterministic function"
$

== State of optimization -- Memory side-effects
What could be the post-condition for memory-side effect functions ?
Suppose a simple `reduce_add` function that adds all elements of an array
into a single integer, array being defined as `type { i32, i32, ptr }`. Notice that we
use wildcards to represent the input/output elements.
```ll
define T @callback(T, T);
define T @reduce(ptr %array)
```
1. We have that `reduce` reads from memory location defined by `%array`
2. Condition: Does not crash if no memory location at `%array..%array+12` and
  at `*(%array + 8)..*(%array + 8 + 4 * ( *(%array + 4) - 1))` are valid
2.a. Condition of concurrency: No other function modifies those memory locations during execution
3. Post-condition: returns the sum of all elements in the array

== State of optimization -- HashMap
What could be the post-condition for complex data-structure manipulation functions ?
```ll
@requisite type T, U, h(key) satisfy "hash";
define U @hashmap_get<h>(ptr %hashmap, T key);
define void @hashmap_set<h>(ptr %hashmap, T key, U value);
```

1. Condition on hash. $epsilon$ encapsulate the quality of the hash function~:
$
  forall k_1, k_2 in T. space k_1 != k_2 => \
  P(h(k_1) = h(k_2)) <= 1/(2^64) + epsilon or P(h(k_1) = h(k_2)) <= (1 + epsilon) dot 1/(2^64)
$

#todo[
  Find a way to construct ordering and constraint between multiple function.
]

== State of optimization
As such, we have seen what constraints and conditions we should expect for a good axiomatic framework. Namely~:
1. Should allow for _intermediary assertions_ and *behavior-characterization* of functions. It should also allow loop time analysis.
2. Should allow precondition to capture complex condition for flow disjunction.
3. Should allow for post-condition to capture complex behavior of many-functions.
4. Should account for *non-determinism* and *probabilistic algorithms*.
  - Use of $PP: cal(B) -> RR_[0,1]$ and other statistics such as $EE, "Var"$. This will become the base for statistical modeling during optimisation.
5. Should account for *memory side-effects* and *concurrency* by providing ordering constraints.
  - Memory locations should *always* be tracked, notably assertions should be made about
    *non-interfering* per-location as well as the equivalent for `__restrict__` pointers.
  - Typing inferred from memory _(if possible)_.
  - Keep track of addressing by nested function call (notably for data-structure substitution candidacy).
6. Create a type of *abstracting* contract for easier thinking about program flow without considering
  all memory access (basically a type of aliasing rule with strong preconditions/ordering/concurrency).

#diapositive-break()
Here is a list of elements to consider

#note[
  - Building of _concepts_ which are a group of *preconditions* on potentially generic function.
    *Examples*: `commutative<T, op>`, `hash<T, h>`, `container<C, T, get, set, size>`, etc.
]

#todo[
  - Construct *abstracing* on multi-function, also for concurrency modeling.
  - Modeling of non-determinism and probabilistic algorithms.
  - Add quantifier forall using `assumption` and `assertion`.
  - Existence as well ?
]

#proposition(title: "Non-Determinism")[
  We define the probability function $P : Omega -> [0, 1]$ where $Omega$ is the set of all possible outcomes of a non-deterministic function. By definition, $P$ always return in type $RR$. We also define $P("#N" >= 1)$ the probability a label is reached

  For instance we can model the quality of an hash function $h$ as follows~:
  $forall k_1, k_2 in T. space k_1 != k_2 => P(h(k_1) = h(k_2)) <= (1 + epsilon) dot 1/(2^64 - 1)$
]

#proposition(title: "Ordering & Concurrency")[
  We define a ordering relation $prec$ over memory locations such that for any two functions $f, g in bb(F)$ executed concurrently~:
  - If $mu \{f\} union delta \{g\} != emptyset$ or $mu \{g\} union delta \{f\} != emptyset$ then either $f prec g$ or $g prec f$.
  - If $f prec g$ then for any memory location $m in mu \{f\} union delta \{g\}$, all writes to $m$ by $f$ happen before any read or write to $m$ by $g$.
] <ordering-concurrency-proposition>

#todo[
  Refine proposition #ref(<ordering-concurrency-proposition>) to account for more complex synchronization mechanisms.

  - Define synchronious and asynchronious ordering.
  - Define weak-ordering (some operations can be neither before nor after for instance certain effect are seen only after a synchronization point and behavior before is either unchanged or *UB*.)
  - Allow for *UB* modeling in synchronisation.
]


#diapositive-break()

Here is a diagram illustrating the propose architecture that satisfy the above requirements


#align(center, {
  diagram(
    cell-size: (19mm, 10mm),
    spacing: (17pt, 11pt),
    // spacing: (8pt, 1pt),
    mark-scale: 150%,

    // Descriptive level
    blob((0, 0), [Module $m$], tint: purple.darken(20%)),
    blob((0, 1), [Function $f$], tint: blue.darken(20%)),
    blob((0, 2), [Basic Bloc $B_i$], tint: gray.darken(60%)),
    blob((-0.6, 3), [Instruction $I_(i,j)$], tint: gray.darken(60%)),
    blob((0.6, 3), [Terminator $T_i$], tint: gray.darken(60%)),

    node(
      enclose: ((-1.5, -1), (1.1, 4)),
      stroke: teal,
      fill: with_alpha(teal, 10%),
      inset: 8pt,
      place(
        dx: 0em,
        dy: 1.7em,
        bottom + center,
        [
          `HyInstr`, descriptive level
        ],
      ),
    ),

    edge((0, 0), (0, 1), "-<>"),
    edge((0, 1), (0, 2), "-<>"),
    edge((0, 2), (-0.6, 3), "-<>"),
    edge((0, 2), (0.6, 3), "-|>"),

    // Meta-analysis blobs
    blob((2, 0), [ModuleSpec], tint: green.lighten(20%)),
    blob((2, 1), [Specification], tint: green.lighten(21%)),
    blob((3, 1), [FuncAnalysis], tint: red.lighten(21%)),
    blob((4, 1), [Abstraction], tint: orange.lighten(21%)),

    blob((2, 2), [Pre/Post-condition], tint: yellow),
    blob((3, 2), [State Based Analysis], tint: aqua),
    blob((4, 2), [Internals], tint: yellow),

    blob((2, 3), [TBehavior], tint: yellow),
    blob((3, 3), [MemAliasing], tint: yellow),

    edge((2, 0), (2, 1), "-|>"),
    edge((2, 0), (3, 1), "-<>"),

    edge((2, 1), (2, 2), "-<>"),
    edge((2, 1), (3, 2), "-<>"),
    edge((2, 2), (2, 3), "-|>"),
    edge((3, 2), (2, 3), "-|>"),
    edge((2, 2), (3, 3), "-|>"),
    edge((3, 2), (3, 3), "-|>"),

    edge((3, 1), (4, 2), "-<>"),
    edge((3, 1), (3, 2), "-<>"),

    // blob((2, 2), [Postcondition]),
    // blob((3, 2), [Internals]),
    // blob((4, 2), [MemAliasing/Typer]),

    // blob((2, 3), [Precondition]),
    // blob((3, 3), [FuncAnalysis]),
    // blob((4, 3), [MultiFuncCond]),

    // blob((2, 4), [Assertion]),
    // blob((3, 4), [Abstraction]),
    // blob((4, 4), [TermBehavior], tint: blue.lighten(60%)),
  )
})

== Specification
*Specification* provides post-conditions and behavior about a certain function or set of functions. It is used to guide the analysis and abstraction phases.
- Provides information about behavior to expect from a function ($lozenge.filled, lozenge, "completion"$)
  ```ll
  @myhashmap.set terminate/crash on always;
  ```
- Supports multi-function modeling for instance~:
  ```ll
  %p = @myhashmap.get(%hashmap, %index_1);
  %b = @myhashmap.set(%hashmap, %index, %value);
  %a = @myhashmap.get(%hashmap, %index_1);
  assert((%a == %p) and (%index != %index_1) or (%index == %index_1 and %a == %value));
  ```
*NOTE*: _We could also provide a *many* function spec. for multithreading_

#pagebreak()

- Supports abstract properties definition such as *commutativity*, *associativity*, *idempotence*, etc.
  ```ll
  behavior commutative<op, A>(@op, %a, %b) {
    assert(@op(%a, %b) == @op(%b, %a));
  }
  ```
- Supports *constant* function (functions that have non-modifying side-effects in a given argument) + memory aliasing information.
- Memory aliasing should track region or memory read/write (that *could* be).

  #todo[
    Figure it out
  ]

== Finite State Analysis
The *Finite State Analysis* provides a basic framework to analyze concurrent algorithms and functions with complex dependencies.

#definition(title: "FSA")[
  Suppose we have a set of functions $F = {f_1, ..., f_n}$ with interfering memory side-effects. We first define *atoms of a function* as the biggest sequence of operation that are one of the following~:
  - *Rule of exclusion*: Provably non-interfering with any other function in $F$.
  - *Interleaving point*: Point where memory side-effects could interfere with another function in $F$.
]

#todo[
  - Define a framework to analyze such functions based on their atoms.
  - Define a way to extract such atoms from functions.
]

= Conclusion
== Thanks for your attention!
#v(5mm)
Thank you for your attention! Any questions?

#align(center, [
  #qr-code(
    "https://github.com/BoyeGuillaume/hyperion",
    width: 60mm,
    height: 60mm,
    quiet-zone: true,
  )

  #set text(size: 0.9em)
  Visit the Hyperion GitHub Repository at https://github.com/BoyeGuillaume/hyperion
])
